---
title             : "ML4 Results Section in rMarkdown"
shorttitle        : "ML4 Results"

author: 
  - name          : "Richard A. Klein"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : ""
    email         : "raklein22@gmail.com"

affiliation:
  - id            : "1"
    institution   : "Université Grenoble Alpes"

authornote: |
  This script generates the participants + results sections for the main ML4 manuscript. To knit this document you must install the papaja package from GitHub.

abstract: |

keywords          : "Terror Management Theory, mortality salience, replication, many labs"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : yes
linenumbers       : no
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_word
---

```{r setup, include = FALSE}
# load packages
library("papaja")
library("metafor")
library("metaSEM")
library("haven")
library("psych")
library("dplyr")
library("effsize")
library("GPArotation")
library("tidyverse")
library("pwr")

# source functions stored locally
source("sources/numbers2words.r")

firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

# for calling formatC with desired arguments
formatCC <- function(x) formatC(x, big.mark = ",") 

# for reporting results from object of class "meta" in tidy fashion in-line
report_meta <- function(object, term, label = "*Hedges' g*") {
  # grab coefficients table
  res.df <- summary(object)$coefficients
  # fetch model stats
  g <- res.df[term, "Estimate"]
  g.lbound <- res.df[term, "lbound"]
  g.ubound <- res.df[term, "ubound"]
  se <- res.df[term, 'Std.Error']
  z <- res.df[term, 'z value']
  p <- res.df[term, 'Pr(>|z|)']
  
  # paste together into report
  paste0(label, " = ", printnum(g),
         ", 95% CI = [", printnum(g.lbound), ", ", printnum(g.ubound), 
         "], *SE* = ", printnum(se), 
         ", *Z* = ", printnum(z),
         ", *p* = ", printp(p))
}

report_q <- function(object) {
  # grab Q.stat list
  res.df <- summary(object)$Q.stat
  # fetch the numbers
  paste0("*Q*(", res.df$Q.df,
         ") = ", printnum(res.df$Q),
         ", *p* = ", printp(res.df$pval))
}
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(1)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r analysis-loaddata, include = FALSE}
# Reading in all necessary data
# Note: Some analyses require confidential (for participant identification concerns) data files. You'll have to comment out those lines for this file to knit. Contact Rick raklein22@gmail.com for information about getting the raw data, which usually simply requires the researcher to obtain IRB/ethics approval from their university stating they will maintain the confidentiality of any sensitive data.

# Primary data file with replication data aggregated across labs (deidentified is missing just a couple variables for this -- age and gender)
# Note: This file is the merged data provided by sites, produced by 001_data_cleaning.R
merged <- readRDS("./data/processed_data/merged_subset.rds")
# Alternatively, you can run it with the public data and get most results. The problem with RMarkdown is that the script
# needs to run completely with no errors or it won't render. So, if you do this, you'll need to comment out any
# sections that require sensitive data.
#merged <- readRDS("./data/public/merged_deidentified.rds")

# Read in data from experimenter survey (Google Form - private due to sensitive info)
exp_surv <- readRDS("./data/raw_site_data/experimenter survey/exp_surv.rds")
```

```{r analysis-participants, include = FALSE}

# The 'merged' df includes all data provided by sites
# I'm going to retain it in case we need to refer to it later, and then
# apply the study-wide exclusion criteria noted below.

merged_original <- merged

# Aggregate participants characteristics
# Converting to numeric
merged$age <- as.numeric(as.character(merged$age))

# Applying exclusion criteria 1
#    need both the pro and the anti ratings
#    pass_ER1 was created in 001_data_cleaning.R
#    non-expert sites are exempted
merged <- filter(merged, !is.na(pro_minus_anti), pass_ER1 == T | expert == 0)

# fetch demographics and extract values generated in 002_ml4analysis.R
#   gender
demos_gender <- read_csv("./data/processed_data/demos_gender.csv")
n_woman     <- filter(demos_gender, gender == 1) %>% select(n)    %>% as.numeric
n_woman_pct <- filter(demos_gender, gender == 1) %>% select(pct)  %>% as.numeric
n_man       <- filter(demos_gender, gender == 2) %>% select(n)    %>% as.numeric
n_man_pct   <- filter(demos_gender, gender == 2) %>% select(pct)  %>% as.numeric

#   race
demos_race <- read_csv("./data/processed_data/demos_race.csv")
n_white     <- filter(demos_race, race == 1) %>% select(n)    %>% as.numeric
n_white_pct <- filter(demos_race, race == 1) %>% select(pct)  %>% as.numeric
n_black     <- filter(demos_race, race == 2) %>% select(n)    %>% as.numeric
n_black_pct <- filter(demos_race, race == 2) %>% select(pct)  %>% as.numeric
n_aian      <- filter(demos_race, race == 3) %>% select(n)    %>% as.numeric
n_aian_pct  <- filter(demos_race, race == 3) %>% select(pct)  %>% as.numeric
n_asian     <- filter(demos_race, race == 4) %>% select(n)    %>% as.numeric
n_asian_pct <- filter(demos_race, race == 4) %>% select(pct)  %>% as.numeric
n_haw       <- filter(demos_race, race == 5) %>% select(n)    %>% as.numeric
n_haw_pct   <- filter(demos_race, race == 5) %>% select(pct)  %>% as.numeric
n_other     <- filter(demos_race, race == 6) %>% select(n)    %>% as.numeric
n_other_pct <- filter(demos_race, race == 6) %>% select(pct)  %>% as.numeric

# load results of experimenter survey
source("004_exp_survey_analysis.R")
```

## Participants
Twenty-one labs from 18 universities participated in the project and collected data from a total of 2,281 participants.
Several overall exclusions were made, each as specified by our preregistration (https://osf.io/4xx6w). First, data from four labs were excluded for collecting fewer than 60 participants.[^chatardcommentary] This excluded 158 participants. Next, some In House sites began data collection before the analysis plan was pre-registered due to their own deadlines. Data collected before the pre-registration were excluded from the confirmatory tests reported in this manuscript. This excluded a further 545 participants.[^fulldataanalysis] 

From the remaining `r length(unique(merged_original$source))` labs and `r formatCC(nrow(merged_original))` participants, in adherence to our pre-registration, we excluded from all analyses participants who either failed to complete all 6 ratings of the essay authors, or who failed to complete both writing prompts within the mortality salience or control conditions (e.g., the between-subjects manipulation). The latter exclusion criteria applied only to participants from Author Advised sites, because the necessary data was not always available for In House sites. Thus, the usable sample included `r formatCC(nrow(merged))` participants (see Table 1 for a breakdown by lab).

`r formatCC(n_woman)` participants (`r n_woman_pct`%) were women and `r formatCC(n_man)` participants (`r n_man_pct`%) were men; the remaining participants did not respond to the item, were asked about gender in a non-standard way, or chose a different response. The mean age was `r mean(merged$age, na.rm=TRUE)` years (*SD* = `r sd(merged$age, na.rm=TRUE)`). Participant reported race was `r n_white` (`r n_white_pct`%) White, `r n_asian` (`r n_asian_pct`%) Asian, `r n_black` (`r n_black_pct`%) Black or African American, `r n_aian` (`r n_aian_pct`%) American Indian or Alaska Native, `r n_haw` (`r n_haw_pct`%) Native Hawaiian or Pacific Islander, and `r n_other` (`r n_other_pct`%) another category. The remaining participants did not report their race or their responses were not easily recoded to match these categories.

[^chatardcommentary]: We originally posted a pre-print accidentally omitting this exclusion: https://psyarxiv.com/vef2c/. In a critique, Chatard, Hirschberger, and Pyszczynski (2020) noted this as a deviation from the pre-registration. We revised the present manuscript with these comments in mind and ensured strict adherence to the pre-registration.  
[^fulldataanalysis]: Whether these participants are included or excluded has little impact on the results. When these participants are included, some confidence intervals are somewhat narrower, and some results show effect sizes that are slightly smaller. The results presented without these exclusions may be viewed on the OSF page, and all data are available for reanalysis: https://osf.io/8ccnw/  

```{r analysis-alpha-exclusions, include = FALSE}
alpha_anti <- psych::alpha(select(merged, antius3, antius4, antius5))
alpha_pro <- psych::alpha(select(merged, prous3, prous4, prous5))

# Tracking exclusions:
# 'merged_original' is all data, no exclusions
# 'merged' is basic exclusions (exclusion set 1 below). Implemented in the participants code chunk at the beginning.
# 'merged_excl_2' further excludes participants as per exclusion set 2 (below)
merged_excl_2 <- subset(merged, pass_ER2 == T | expert == 0)

# 'merged_excl_3' further excludes participants as per exclusion set 3 (below)
merged_excl_3 <- subset(merged_excl_2, pass_ER3 == T | expert == 0)
```

## Analysis Plan
The primary finding of interest from Greenberg et al. (1994) was that participants who underwent the mortality salience treatment showed greater preference for the pro-US essay author over the anti-US essay author as compared to the control condition. To assess whether the replication results support the original, within each lab we followed a similar analysis plan as in the original article, as specified by our pre-registration (https://osf.io/4xx6w). Scores from the three items evaluating the authors of the anti-American essays were averaged ($\alpha$ = `r alpha_anti$total$std.alpha`) and then subtracted from the average of the three items evaluating authors of the pro-American essays ($\alpha$ = `r alpha_pro$total$std.alpha`). An independent-samples *t*-test was then conducted comparing those in the “subtle own death salient” (MS) condition with scores from the “TV salient” (control) condition. We then analyzed these individual results meta-analytically to get an aggregate effect size across all labs. Supplemental exploratory (non-preregistered) analyses treating these as two separate dependent variables are available in the online supplement (https://osf.io/xtg4u/), and those outcomes do not qualify the conclusions offered here. 

In addition to the overall exclusions detailed previously in the “Participants” section, original authors requested some additional exclusions based on participant responses. Original authors were not entirely in agreement about what exclusions should be implemented. Therefore, as indicated in the pre-registration, we repeated our analyses under three different exclusion criteria: A minimal set of exclusions (Exclusion Set 1), and two sets of more strict exclusions recommended by original authors (Exclusion Sets 2 and 3). 

Because Exclusion Sets 2 and 3 were specifically recommended by original authors and could be considered part of their expertise, we did not inform the In House labs about these exclusions prior to data collection. This masking was to avoid influencing their decisions about data collection procedures, which had to be made independently of any outside help. However, as a result, none of the In House labs collected the data required to make these more strict exclusions because the items required were not present in the original target article. 

Therefore, analyses were repeated for each of the three exclusion sets for Author Advised participants. However, Exclusion Set 1 was used consistently across all analyses for In House participants. The exclusion sets consisted of:

*Exclusion Set 1:* Exclude participants who did not complete both writing prompts and all six items evaluating the essay authors. This yields `r formatCC(nrow(merged))` participants (*n* = `r formatCC(merged %>% filter(expert == 1) %>% nrow())` Author Advised, *n* = `r formatCC(merged %>% filter(expert == 0) %>% nrow())` In House). This aggregate sample size gives us 95% power to detect a mortality salience condition effect of *d* = `r pwr::pwr.t.test(n = nrow(merged)/2, power = .95, type = "two.sample", alternative = "two.sided")$d %>% round(2)` in an independent samples *t*-test.[^powernote]  

*Exclusion Set 2:* All prior exclusions, and further exclude participants who did not identify as White or who indicated they were born outside the United States. This reduces the *N* to `r formatCC(nrow(merged_excl_2))` participants (*n* = `r formatCC(merged_excl_2 %>% filter(expert == 1) %>% nrow())` Author Advised, *n* = `r formatCC(merged_excl_2 %>% filter(expert == 0) %>% nrow())` In House). This aggregate sample size gives us 95% power to detect a mortality salience condition effect of *d* = `r pwr::pwr.t.test(n = nrow(merged_excl_2)/2, power = .95, type = "two.sample", alternative = "two.sided")$d %>% round(2)`.  

*Exclusion Set 3:* All prior exclusions, and further exclude participants who responded lower than 7 on the American Identity item ("How important to you is your identity as an American?" 1 - not at all important; 9 - extremely important). This further reduces the usable *N* to `r formatCC(nrow(merged_excl_3))` participants (*n* = `r formatCC(merged_excl_3 %>% filter(expert == 1) %>% nrow())` Author Advised, *n* = `r formatCC(merged_excl_3 %>% filter(expert == 0) %>% nrow())` In House). This aggregate sample size gives us 95% power to detect a mortality salience condition effect of *d* = `r pwr::pwr.t.test(n = nrow(merged_excl_3)/2, power = .95, type = "two.sample", alternative = "two.sided")$d %>% round(2)`.  

All data handling, exclusions, and computation of results within sites followed our pre-registered (prior to data collection) analysis plan on the OSF (https://osf.io/4xx6w). All results are reported with two-tailed tests of significance.[^signote]

[^powernote]: Power calculated with the 'pwr' R package (Champely, 2020) assuming equal distribution of participants between mortality salience and control conditions, and two-tailed test with alpha of .05..

[^signote]: We did not specifically pre-register whether tests would be one- or two-tailed, and some have argued that one-tailed tests would be more appropriate (e.g., Chatard, Hirschberger, & Pyszczynski, 2020). However, we had always intended to use two-tailed tests and this is consistent with all prior Many Labs projects (Klein et al., 2014; 2018, Ebersole et al., 2016; 2020) and the pre-registered sample analysis code (https://osf.io/4xx6w).

# Results

```{r analysis-replication-meta, include = FALSE}

load("Results.RData")

# Notes: Intercept1 is still the grand mean estimate, 
# Slope1_1 represents the effect of expertise
# Tau2_1_1 is the heterogeneity estimate
```

## Deviations from Pre-registered Analytic Plan  
Our pre-registered analytic plan specifies the use of a three-level meta-analysis, conducted in the MetaSEM R package (Cheung, 2014), to control for the clustering of effect sizes when independent teams ran both In House and Author Advised versions of the protocol at the same university. However, during data analysis we discovered that these models failed to converge because we did not have enough data within each cluster, as most sites conducted only one study. As such, we had to drop the clustering variable. The results reported below are thus a more common univariate meta-analysis, which is the model that most closely mirrors our originally planned analysis.

## Research Question 1: Meta-analytic results across all labs (random effects meta-analysis). 
The most basic question is whether we observed the predicted effect of mortality salience on preference for pro- vs anti- American essay authors. To assess this, we conducted a random-effects meta-analysis. This analysis produces the grand mean effect size across all sites and versions. Regardless of which exclusion criteria were used, we did not observe the predicted effect, and the confidence interval was quite narrow: 
Exclusion Set 1: `r report_meta(random1, "Intercept1")`. 
Exclusion Set 2: `r report_meta(random2, "Intercept1")`. 
Exclusion Set 3: `r report_meta(random3, "Intercept1")`. 
Forest plots showing the effects for individual sites and the aggregate are available in Figure 1 for Exclusion Set 1. (See https://osf.io/8ccnw/ for the other two Exclusion Sets.) These results indicate that, in the aggregate, we failed to replicate the predicted mortality salience effect.

There may have been a mortality salience effect at some sites and not others, so we next examined how much variation was observed among effect sizes (e.g., heterogeneity). This sort of variation did not exceed variation expected by chance (e.g., sampling variance) regardless of exclusion rule: 
Exclusion Set 1: `r report_q(random1)`; 
Exclusion Set 2: `r report_q(random2)`; 
Exclusion Set 3: `r report_q(random3)`. 
This result suggests that any observed differences in effect size between sites are likely due to random noise, as opposed to true differences in underlying effect size.

In sum, we observed little evidence for an overall effect of mortality salience in these replications. Additionally, overall results suggest that there was little or no heterogeneity in effect sizes across sites. This lack of variation suggests that it is unlikely we will observe an effect of Author Advised versus In House protocols or other moderators such as differences in samples or TMT knowledge. Even so, the plausible moderation by Author Advised/In House protocol is examined in the following section. 

## Research Question 2: Moderation by Author Advised/In House protocol
A covariate of protocol type (In House vs Author Advised) was added to the random effects model to create a mixed-effects meta-analysis. This is our primary model of interest, and the model most similar to the three-level mixed-effects meta-analysis that we pre-registered as our primary outcome.

This analysis again produces an overall grand mean effect size, and those were again near zero and relatively precisely estimated across all three Exclusion Sets: 
Exclusion Set 1: `r report_meta(mixed1, "Intercept1")`. 
Exclusion Set 2: `r report_meta(mixed2, "Intercept1")`. 
Exclusion Set 3: `r report_meta(mixed3, "Intercept1")`.

Again, significant heterogeneity was not observed: 
Exclusion Set 1, `r report_q(mixed1)`; 
Exclusion Set 2, `r report_q(mixed2)`; 
Exclusion Set 3, `r report_q(mixed3)`.
This again suggests that any observed differences in effect size between sites are likely due to random noise, as opposed to true differences in effect size.

Critically, protocol version did not significantly predict replication effect size regardless of which exclusion criteria were used. 
Exclusion Set 1: `r report_meta(mixed1, "Slope1_1", label = "*b*")`; 
Exclusion Set 2: `r report_meta(mixed2, "Slope1_1", label = "*b*")`;
Exclusion Set 3: `r report_meta(mixed3, "Slope1_1", label = "*b*")`. The Author Advised version did not produce significantly larger effect sizes when compared with the In House versions.

## Research Question 3: Effect of Standardization
Finally, we examined whether In House protocols displayed greater variability in effect size than Author Advised protocols. We outlined this hypothesis in our pre-registration, but the methods for testing it are exploratory.  

As an initial test, we conducted separate meta-analyses for the In House and Author Advised labs. For each, we conducted both a fixed-effects (with variance between labs constrained to be equal to zero) and random-effects meta-analysis, and then compared the two models with a chi-squared differences test to assess whether the fit significantly changed. If the random-effects model fit significantly better than the fixed-effects model, this would indicate that allowing for variability in effect sizes between sites improved the model.

In this case, neither In House nor Author Advised labs showed a significant benefit of the random effects model over the fixed effects model across any of the Exclusion Sets: 
In House labs: 
Exclusion Set 1: *$\chi$²* (`r fit_comparison_1_ih$diffdf[2]`) = `r fit_comparison_1_ih$diffLL[2]`, *p* = `r fit_comparison_1_ih$p[2]`; 
Author Advised labs: 
Exclusion Set 1: *$\chi$²* (`r fit_comparison_1_aa$diffdf[2]`) = `r fit_comparison_1_aa$diffLL[2]`, *p* = `r fit_comparison_1_aa$p[2]`; 
Exclusion Set 2: *$\chi$²* (`r fit_comparison_2_aa$diffdf[2]`) = `r fit_comparison_2_aa$diffLL[2]`, *p* = `r fit_comparison_2_aa$p[2]`; 
Exclusion Set 3: *$\chi$²* (`r fit_comparison_3_aa$diffdf[2]`) = `r fit_comparison_3_aa$diffLL[2]`, *p* = `r fit_comparison_3_aa$p[2]`. 
Overall, this evidence indicates that neither In House nor Author Advised labs showed significant variability in effect size across sites, despite the fact that In House labs were unambiguously more variable in their procedural implementation. This does not mean the variances were equal, but based on the present evidence we cannot conclude that they were different.

## Follow-Up Exploratory (non-preregistered) Analyses
Results reported in this section were not pre-registered and should be considered exploratory.

**Researcher expectations and characteristics**
A total of `r nrow(exp_surv)` researchers from `r length(unique(exp_surv$Site))` participating sites completed an experimenter survey about their motivations and expertise. This survey was administered during data collection, and although no researcher had access to overall project-wide results, about one third of the researchers reported looking at or analyzing their own site’s data prior to completing the survey. Psychology research experience ranged from `r min(exp_surv$years_exp)` to `r max(exp_surv$years_exp)` years (*M* = `r mean(exp_surv$years_exp)`, *SD* = `r sd(exp_surv$years_exp)`). 
`r firstup(numbers2words(exp_knowl_alot))` (`r round(exp_knowl_alot_pct)`%) indicated they had “a lot” of TMT knowledge, `r numbers2words(exp_knowl_some)` (`r round(exp_knowl_some_pct)`%) indicated “some” knowledge, `r numbers2words(exp_knowl_alittle)` (`r round(exp_knowl_alittle_pct)`%) indicated little knowledge, `r numbers2words(exp_knowl_none)` (`r round(exp_knowl_none_pct)`%) indicated zero knowledge, and `r numbers2words(exp_knowl_na)` (`r round(exp_knowl_na_pct)`%) did not respond to the question. 
One researcher indicated that they were an expert in TMT, but their site did not reach the minimum sample size specified by the preregistration.
<!-- TMT expert must have been excluded due to n<60.
r firstup(numbers2words(exp_knowl_expert)) (r round(exp_knowl_expert_pct)%) researcher indicated they were an expert in TMT, -->

When asked what outcome they wanted to happen, `r numbers2words(tab.rooting["For TMT"])` (`r round(tab.rooting_pct["For TMT"])`%) indicated that they hoped for the project to successfully replicate the TMT effect, `r numbers2words(tab.rooting["Neither"])` (`r round(tab.rooting_pct["Neither"])`%) indicated no preference, and `r numbers2words(tab.rooting["Against TMT"])` (`r round(tab.rooting_pct["Against TMT"])`%) hoped the project would result in a failure to replicate, with `r numbers2words(tab.rooting[4])` (`r round(tab.rooting_pct[4])`%) researchers leaving the question blank. On average, the teams estimated a `r round(mean(exp_surv$rep_likely, na.rm = TRUE))`% chance of successful replication with a wide range of estimates from `r min(exp_surv$rep_likely, na.rm = TRUE)`% to `r max(exp_surv$rep_likely, na.rm = TRUE)`% (*SD* = `r sd(exp_surv$rep_likely, na.rm = TRUE)`).[^1]

[^1]: Including only sites that had not looked at any data, researchers still estimated a `r round(mean(exp_surv$rep_likely[exp_surv$analyzed == "No"], na.rm = T))`% chance of successful replication.

**Results for expert-only protocols.** To provide a test of the replicability and average effect size of TMT under ideal circumstances, one could focus only on the effect size within author-advised protocols. Effect sizes are descriptively larger among these sites but still not statistically significant. 
Exclusion rule 1: `r report_meta(random1_aa, "Intercept1")`;
Exclusion rule 2: `r report_meta(random2_aa, "Intercept1")`;
Exclusion rule 3: `r report_meta(random3_aa, "Intercept1")`.

**Results for TMT-knowledgeable sites.** Five principal investigators indicated having “a lot” of knowledge about TMT. One might expect that these locations would have greater success at replicating the mortality salience effect. 
However, in all exclusion sets, when restricting analyses to these five sites, results were not statistically significant:
Exclusion rule 1: `r report_meta(random1_TMTexperienced, "Intercept1")`;
Exclusion rule 2: `r report_meta(random2_TMTexperienced, "Intercept1")`;
Exclusion rule 3: `r report_meta(random3_TMTexperienced, "Intercept1")`.

```{r analysis-exploratory-preferpro, include = FALSE}
# Investigating only participants who reported preference for the pro-US author.
merged <- mutate(merged, 
                 author_pref = case_when(pro_minus_anti > 0 ~ "Favors Pro-US",
                                         pro_minus_anti < 0 ~ "Favors Anti-US",
                                         pro_minus_anti == 0 ~ "No preference"))
with(merged, table(author_pref))
tab_authorpref <- with(merged, table(author_pref, expert))
authorpref_test <- chisq.test(tab_authorpref)

# Proportions of pro-US preference, in-house designs
merged_ih <- filter(merged, expert == 0)

n_profav_ih  <- sum(merged_ih$pro_minus_anti > 0)
n_antifav_ih <- sum(merged_ih$pro_minus_anti < 0)
n_nofav_ih   <- sum(merged_ih$pro_minus_anti == 0)

pct_profav_ih  <- n_profav_ih  / (n_profav_ih + n_antifav_ih + n_nofav_ih) * 100
pct_antifav_ih <- n_antifav_ih / (n_profav_ih + n_antifav_ih + n_nofav_ih) * 100
pct_nofav_ih   <- n_nofav_ih   / (n_profav_ih + n_antifav_ih + n_nofav_ih) * 100

# Porportions of pro-US preference, Author Advised designs
merged_aa <- filter(merged, expert == 1)

n_profav_aa <- sum(merged_aa$pro_minus_anti > 0)
n_antifav_aa <- sum(merged_aa$pro_minus_anti < 0)
n_nofav_aa <- sum(merged_aa$pro_minus_anti == 0)

pct_profav_aa <- (n_profav_aa/(n_profav_aa+n_antifav_aa+n_nofav_aa))*100
pct_antifav_aa <- (n_antifav_aa/(n_profav_aa+n_antifav_aa+n_nofav_aa))*100
pct_nofav_aa <- (n_nofav_aa/(n_profav_aa+n_antifav_aa+n_nofav_aa))*100
```

**Results for participants who preferred the pro-US author** The present hypothesis that mortality salience would cause a participant to become more favorable to the pro-US author as compared to the anti-US author relies on the participant perceiving the pro-US stance as more similar to their own worldview (and/or the anti-US stance as threatening to their worldview). Original authors anticipated that the essays from the original study may not serve this function in the replication, run in 2016. There was a particular concern that in the months leading up to and following the 2016 US Presidential Election of Donald Trump, the generally more liberal-leaning student bodies on college campuses may feel less patriotic and not identify with the pro-US worldview. For this reason, the anti-US essay from the original study was made more extreme in the Author Advised version of the replication. 

This did successfully increase participant preferences for the pro-US author over the anti-US author among Author Advised replications as compared to In House replications, $\chi^2$(`r authorpref_test$parameter`) = `r round(authorpref_test$statistic, digits = 2)`, `r printp(authorpref_test$p.value)`. Among In House replications, `r round(pct_profav_ih, digits = 0)`% of participants preferred the pro-US essay author, `r round(pct_antifav_ih, digits = 0)`% preferred the anti-US essay author, and `r round(pct_nofav_ih, digits = 0)`% had no preference. Among Author Advised replications, `r round(pct_profav_aa, digits = 0)`% of participants preferred the pro-US essay author, `r round(pct_antifav_aa, digits = 0)`% preferred the anti-US essay author, and `r round(pct_nofav_aa, digits = 0)`% had no preference. 

Is the anticipated effect stronger among participants who preferred the pro-US author? We restricted the analysis to only participants at Author Advised sites who preferred the pro-US author. Under this restriction, there was still no significant difference between the mortality salience and control groups in their preference for the pro-US author over the anti-US author. 
Exclusion Set 1: `r report_meta(fixed1_proUSonly, "Intercept1")`; 
Exclusion Set 2: `r report_meta(fixed2_proUSonly, "Intercept1")`; 
Exclusion Set 3: `r report_meta(fixed3_proUSonly, "Intercept1")`.[^2] 

[^2]: The random-effects meta-analysis failed to converge because the heterogeneity parameter, $\tau$, was estimated as very, very small. To get the model to converge, we restricted $\tau$ to zero, creating a fixed-effects meta-analysis.


\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
